{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import numpy as np\n",
    "plt.rcParams['font.sans-serif']=['SimHei'] #用来正常显示中文标签]\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)  # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13380/4033203392.py:3: UserWarning: Parsing dates in DD/MM/YYYY format when dayfirst=False (the default) was specified. This may lead to inconsistently parsed dates! Specify a format to ensure consistent parsing.\n",
      "  df['date']=pd.to_datetime(df['date'])\n"
     ]
    },
    {
     "data": {
      "text/plain": "        date  Doy  Daily_RMSE  GPT2w_ZTD  GAMIT_ZTD       Int      Rand\n0 2009-02-11  306     0.16891  -1.671140   0.411879  0.345004  0.650788\n1 2009-03-11  307     0.19556  -1.631435   0.418651  0.345004  0.412306\n2 2009-04-11  308     0.19142  -1.590069   0.446279  0.345004  0.688613\n3 2009-05-11  309     0.19040  -1.547070   0.383870  0.345004  0.031724\n4 2009-06-11  310     0.18975  -1.502468   0.437124  0.345004  0.526168",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>Doy</th>\n      <th>Daily_RMSE</th>\n      <th>GPT2w_ZTD</th>\n      <th>GAMIT_ZTD</th>\n      <th>Int</th>\n      <th>Rand</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2009-02-11</td>\n      <td>306</td>\n      <td>0.16891</td>\n      <td>-1.671140</td>\n      <td>0.411879</td>\n      <td>0.345004</td>\n      <td>0.650788</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2009-03-11</td>\n      <td>307</td>\n      <td>0.19556</td>\n      <td>-1.631435</td>\n      <td>0.418651</td>\n      <td>0.345004</td>\n      <td>0.412306</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2009-04-11</td>\n      <td>308</td>\n      <td>0.19142</td>\n      <td>-1.590069</td>\n      <td>0.446279</td>\n      <td>0.345004</td>\n      <td>0.688613</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2009-05-11</td>\n      <td>309</td>\n      <td>0.19040</td>\n      <td>-1.547070</td>\n      <td>0.383870</td>\n      <td>0.345004</td>\n      <td>0.031724</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2009-06-11</td>\n      <td>310</td>\n      <td>0.18975</td>\n      <td>-1.502468</td>\n      <td>0.437124</td>\n      <td>0.345004</td>\n      <td>0.526168</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('Daily_RMSE.csv')\n",
    "df['date']=pd.to_datetime(df['date'])\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T20:47:44.510566294Z",
     "start_time": "2023-12-20T20:47:44.387080146Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: SimHei\n"
     ]
    }
   ],
   "source": [
    "plt.plot(df['date'],df['Daily_RMSE'],marker='o',linestyle='--',color='r')\n",
    "# 设置X轴日期格式\n",
    "date_format = mdates.DateFormatter('%Y-%m-%d')  # 设置日期格式\n",
    "plt.gca().xaxis.set_major_formatter(date_format)\n",
    "\n",
    "# 自动调整刻度间距以避免重叠\n",
    "plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=10))  # 间隔一个月显示一个刻度\n",
    "\n",
    "# 可选：旋转刻度标签，以防止它们重叠\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.title('X轴日期格式示例')\n",
    "plt.xlabel('日期')\n",
    "plt.ylabel('数值')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T20:47:50.557695633Z",
     "start_time": "2023-12-20T20:47:50.299282338Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 创建一个示例的多变量时间序列数据集，长度为 1680，输入特征数为 3，输出为 1\n",
    "# 这里使用随机数据作为示例\n",
    "seq_length = 1680\n",
    "input_size = 3\n",
    "output_size = 1\n",
    "\n",
    "# 生成示例数据\n",
    "import pandas as pd\n",
    "df=pd.read_csv('Daily_RMSE.csv')\n",
    "df.head()\n",
    "column_names=['Doy','GPT2w_ZTD','Daily_RMSE']\n",
    "Input=df[column_names]\n",
    "Output=df['GAMIT_ZTD']\n",
    "data=np.array(Input)\n",
    "target=np.array(Output)\n",
    "# data = np.random.randn(seq_length, input_size)\n",
    "# target = np.random.randn(seq_length, output_size)\n",
    "\n",
    "# 将数据转换为 PyTorch 张量，并移动到GPU\n",
    "data_tensor = torch.FloatTensor(data).to('cuda')\n",
    "target_tensor = torch.FloatTensor(target).to('cuda')\n",
    "\n",
    "# 定义一个简单的多层感知器 (MLP) 模型\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# 初始化模型并将其移动到GPU\n",
    "hidden_size = 64  # 隐藏层神经元数量\n",
    "model = MLP(input_size, hidden_size, output_size).to('cuda')\n",
    "\n",
    "# 定义损失函数\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 存储每时刻的 MAE 和 MSE\n",
    "train_mae_values = []\n",
    "train_mse_values = []\n",
    "\n",
    "# 存储每时刻的预测 MAE 和 MSE\n",
    "predicted_mae_values = []\n",
    "predicted_mse_values = []\n",
    "\n",
    "# 存储每时刻的偏差（bias）\n",
    "train_bias_values = []\n",
    "predicted_bias_values = []\n",
    "\n",
    "# 存储损失函数值\n",
    "loss_values = []\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    mae_sum = 0.0\n",
    "    mse_sum = 0.0\n",
    "    bias_sum = 0.0\n",
    "\n",
    "    # 前向传播和反向传播\n",
    "    for i in range(seq_length):\n",
    "        inputs = data_tensor[i:i+1]\n",
    "        targets = target_tensor[i:i+1]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 计算训练结果的 MAE 和 MSE\n",
    "        mae = torch.abs(outputs - targets).mean().item()\n",
    "        mse = ((outputs - targets) ** 2).mean().item()\n",
    "\n",
    "        mae_sum += mae\n",
    "        mse_sum += mse\n",
    "\n",
    "        # 计算训练结果的偏差（bias）\n",
    "        bias = (outputs - targets).mean().item()\n",
    "        bias_sum += bias\n",
    "\n",
    "        # 存储每时刻的 MAE 和 MSE\n",
    "        train_mae_values.append(mae)\n",
    "        train_mse_values.append(mse)\n",
    "        train_bias_values.append(bias)\n",
    "\n",
    "    # 计算平均 MAE 和 MSE\n",
    "    average_mae = mae_sum / seq_length\n",
    "    average_mse = mse_sum / seq_length\n",
    "    average_bias = bias_sum / seq_length\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Average MAE: {average_mae:.4f}, Average MSE: {average_mse:.4f}, Average Bias: {average_bias:.4f}')\n",
    "\n",
    "    loss_values.append(loss.item())\n",
    "\n",
    "# 使用模型进行预测并计算预测结果的 MAE 和 MSE\n",
    "predicted_outputs = []\n",
    "predicted_mae_sum = 0.0\n",
    "predicted_mse_sum = 0.0\n",
    "predicted_bias_sum = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(seq_length):\n",
    "        test_input = data_tensor[i:i+1]\n",
    "        predicted_output = model(test_input)\n",
    "\n",
    "        # 计算预测结果的 MAE 和 MSE\n",
    "        predicted_mae = torch.abs(predicted_output - target_tensor[i:i+1]).mean().item()\n",
    "        predicted_mse = ((predicted_output - target_tensor[i:i+1]) ** 2).mean().item()\n",
    "\n",
    "        predicted_mae_sum += predicted_mae\n",
    "        predicted_mse_sum += predicted_mse\n",
    "\n",
    "        # 计算预测结果的偏差（bias）\n",
    "        predicted_bias = (predicted_output - target_tensor[i:i+1]).mean().item()\n",
    "        predicted_bias_sum += predicted_bias\n",
    "\n",
    "        # 存储每时刻的预测 MAE 和 MSE\n",
    "        predicted_mae_values.append(predicted_mae)\n",
    "        predicted_mse_values.append(predicted_mse)\n",
    "        predicted_bias_values.append(predicted_bias)\n",
    "\n",
    "    # 计算平均预测 MAE 和 MSE\n",
    "    average_predicted_mae = predicted_mae_sum / seq_length\n",
    "    average_predicted_mse = predicted_mse_sum / seq_length\n",
    "    average_predicted_bias = predicted_bias_sum / seq_length\n",
    "\n",
    "# 绘制每时刻的 MAE 和 MSE 曲线\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(train_mae_values, label='Train MAE', color='blue')\n",
    "plt.plot(predicted_mae_values, label='Predicted MAE', color='red')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.title('MAE vs Time Step')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(train_mse_values, label='Train MSE', color='blue')\n",
    "plt.plot(predicted_mse_values, label='Predicted MSE', color='red')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.title('MSE vs Time Step')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 打印和绘制损失函数的曲线\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(range(num_epochs), loss_values, label='Loss', color='green')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss vs Epoch')\n",
    "plt.show()\n",
    "\n",
    "# 打印训练结果和预测结果的偏差（bias）\n",
    "print(f'Training Bias: {average_bias:.4f}, Predicted Bias: {average_predicted_bias:.4f}')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "plt.plot(train_mae_values[:1680], label='Predicted MSE', color='red')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "##Transformer model for time series prediction without error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Simulated time series data\n",
    "seq_length = 100\n",
    "time = np.linspace(0, 10, seq_length)\n",
    "data = np.sin(time) + np.random.normal(0, 0.1, seq_length)\n",
    "\n",
    "# Convert data to PyTorch tensor\n",
    "data_tensor = torch.FloatTensor(data).unsqueeze(1)\n",
    "\n",
    "# Prepare input and target data\n",
    "input_seq = data_tensor[:-1]\n",
    "target_seq = data_tensor[1:]\n",
    "\n",
    "# Define the Transformer model\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_size, d_model, output_size):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, d_model)\n",
    "        self.transformer = nn.Transformer(d_model, nhead=2, num_encoder_layers=2)\n",
    "        self.output = nn.Linear(d_model, output_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src = self.fc(src)\n",
    "        tgt = self.fc(tgt)\n",
    "        out = self.transformer(src, tgt)\n",
    "        out = self.output(out)\n",
    "        return out\n",
    "\n",
    "# Create the model and define loss and optimizer\n",
    "input_size = 1\n",
    "d_model = 64\n",
    "output_size = 1\n",
    "model = TransformerModel(input_size, d_model, output_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training\n",
    "num_epochs = 500\n",
    "losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(input_seq, input_seq)\n",
    "    loss = criterion(outputs, target_seq)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "\n",
    "# Prediction\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predicted = model(input_seq, input_seq)\n",
    "\n",
    "# Plotting\n",
    "plt.plot(time[1:], data[1:], label=\"Actual Data\")\n",
    "plt.plot(time[1:], predicted, label=\"Predicted Data\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Simulate time series data\n",
    "seq_length = 1680\n",
    "input_size = 3\n",
    "output_size = 1\n",
    "\n",
    "# Generate example data\n",
    "df=pd.read_csv('Daily_RMSE.csv')\n",
    "column_names=['Doy','GPT2w_ZTD','Daily_RMSE']\n",
    "Input=df[column_names]\n",
    "Output=df['GAMIT_ZTD']\n",
    "data=np.array(Input)\n",
    "target=np.array(Output)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "data_tensor = torch.FloatTensor(data)\n",
    "target_tensor = torch.FloatTensor(target)\n",
    "\n",
    "# Move data and model to GPU (if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data_tensor = data_tensor.to(device)\n",
    "target_tensor = target_tensor.to(device)\n",
    "\n",
    "# Define a Transformer model\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_size, d_model, output_size):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.transformer = nn.Transformer(d_model, nhead=2, num_encoder_layers=2)\n",
    "        self.fc = nn.Linear(input_size, d_model)\n",
    "        self.output = nn.Linear(d_model, output_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src = self.fc(src)\n",
    "        tgt = self.fc(tgt)\n",
    "        out = self.transformer(src, tgt)\n",
    "        out = self.output(out)\n",
    "        return out\n",
    "\n",
    "# Initialize the model and move it to the GPU\n",
    "d_model = 64  # Transformer's dimension\n",
    "model = TransformerModel(input_size, d_model, output_size).to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Lists to store metrics\n",
    "mae_values = []\n",
    "mse_values = []\n",
    "bias_values = []\n",
    "loss_values = []\n",
    "\n",
    "# Training the model\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    mae_sum = 0.0\n",
    "    mse_sum = 0.0\n",
    "    bias_sum = 0.0\n",
    "\n",
    "    for i in range(seq_length):\n",
    "        inputs = data_tensor[i:i+1]\n",
    "        targets = target_tensor[i:i+1]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, inputs)  # Use the same input for both src and tgt\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate MAE and MSE\n",
    "        mae = torch.abs(outputs - targets).mean().item()\n",
    "        mse = ((outputs - targets) ** 2).mean().item()\n",
    "\n",
    "        mae_sum += mae\n",
    "        mse_sum += mse\n",
    "\n",
    "        # Calculate bias\n",
    "        bias = (outputs - targets).mean().item()\n",
    "        bias_sum += bias\n",
    "\n",
    "        # Store MAE and MSE at each time step\n",
    "        mae_values.append(mae)\n",
    "        mse_values.append(mse)\n",
    "\n",
    "    # Calculate average MAE and MSE\n",
    "    average_mae = mae_sum / seq_length\n",
    "    average_mse = mse_sum / seq_length\n",
    "    average_bias = bias_sum / seq_length\n",
    "\n",
    "    loss_values.append(loss.item())\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Average MAE: {average_mae:.4f}, Average MSE: {average_mse:.4f}, Average Bias: {average_bias:.4f}')\n",
    "\n",
    "# Use the model for prediction and calculate MAE, MSE, and bias\n",
    "predicted_outputs = []\n",
    "predicted_mae_sum = 0.0\n",
    "predicted_mse_sum = 0.0\n",
    "predicted_bias_sum = 0.0\n",
    "predicted_mae_concate=[]\n",
    "predicted_mse_concate=[]\n",
    "with torch.no_grad():\n",
    "    for i in range(seq_length):\n",
    "        test_input = data_tensor[i:i+1]\n",
    "        predicted_output = model(test_input, test_input)\n",
    "\n",
    "        # Calculate MAE, MSE, and bias for the predicted outputs\n",
    "        predicted_mae = torch.abs(predicted_output - target_tensor[i:i+1]).mean().item()\n",
    "        predicted_mse = ((predicted_output - target_tensor[i:i+1]) ** 2).mean().item()\n",
    "\n",
    "        predicted_mae_sum += predicted_mae\n",
    "        predicted_mse_sum += predicted_mse\n",
    "        predicted_mae_concate.append(predicted_mae)\n",
    "        predicted_mse_concate.append(predicted_mse)\n",
    "        predicted_bias = (predicted_output - target_tensor[i:i+1]).mean().item()\n",
    "        predicted_bias_sum += predicted_bias\n",
    "\n",
    "# Calculate average MAE, MSE, and bias for the predicted outputs\n",
    "average_predicted_mae = predicted_mae_sum / seq_length\n",
    "average_predicted_mse = predicted_mse_sum / seq_length\n",
    "average_predicted_bias = predicted_bias_sum / seq_length\n",
    "\n",
    "# Plot MAE and MSE at each time step\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(mae_values, label='MAE', color='blue')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.title('MAE vs Time Step')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(mse_values, label='MSE', color='red')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.title('MSE vs Time Step')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 打印和绘制损失函数的曲线\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(loss_values, label='Loss', color='green')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss vs Epoch')\n",
    "plt.show()\n",
    "\n",
    "# 打印训练结果和预测结果的偏差（bias）\n",
    "print(f'Training Bias: {average_bias:.4f}')\n",
    "print(f'Predicted Bias: {average_predicted_bias:.4f}')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(mae_values[:1680], label='MAE', color='blue')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.title('MAE vs Time Step')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(mse_values[:1680], label='MSE', color='red')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.title('MSE vs Time Step')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 设置随机种子以便结果可重复\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# 模拟时间序列数据\n",
    "seq_length = 1680\n",
    "input_size = 3\n",
    "output_size = 1\n",
    "\n",
    "# 生成示例数据\n",
    "# Generate example data\n",
    "df=pd.read_csv('Daily_RMSE.csv')\n",
    "column_names=['Doy','GPT2w_ZTD','Daily_RMSE']\n",
    "Input=df[column_names]\n",
    "Output=df['GAMIT_ZTD']\n",
    "data=np.array(Input)\n",
    "target=np.array(Output)\n",
    "\n",
    "# 将数据转换为PyTorch张量\n",
    "data_tensor = torch.FloatTensor(data)\n",
    "target_tensor = torch.FloatTensor(target)\n",
    "\n",
    "# 将数据移动到GPU（如果可用）\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data_tensor = data_tensor.to(device)\n",
    "target_tensor = target_tensor.to(device)\n",
    "\n",
    "# 定义LSTM模型\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])  # 使用最后一个时间步的输出\n",
    "        return out\n",
    "\n",
    "# 初始化模型\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "model = LSTMModel(input_size, hidden_size, output_size, num_layers).to(device)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 存储每时刻的 MAE 和 MSE\n",
    "mae_values = []\n",
    "mse_values = []\n",
    "\n",
    "# 存储每时刻的偏差（bias）\n",
    "bias_values = []\n",
    "\n",
    "# 存储损失函数值\n",
    "loss_values = []\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    mae_sum = 0.0\n",
    "    mse_sum = 0.0\n",
    "    bias_sum = 0.0\n",
    "\n",
    "    for i in range(seq_length):\n",
    "        inputs = data_tensor[i:i+1].unsqueeze(0)  # 添加unsqueeze(0)以匹配模型的输入维度\n",
    "        targets = target_tensor[i:i+1]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 计算 MAE 和 MSE\n",
    "        mae = torch.abs(outputs - targets).mean().item()\n",
    "        mse = ((outputs - targets) ** 2).mean().item()\n",
    "\n",
    "        mae_sum += mae\n",
    "        mse_sum += mse\n",
    "\n",
    "        # 计算偏差（bias）\n",
    "        bias = (outputs - targets).mean().item()\n",
    "        bias_sum += bias\n",
    "\n",
    "        # 存储每时刻的 MAE 和 MSE\n",
    "        mae_values.append(mae)\n",
    "        mse_values.append(mse)\n",
    "\n",
    "    # 计算平均 MAE 和 MSE\n",
    "    average_mae = mae_sum / seq_length\n",
    "    average_mse = mse_sum / seq_length\n",
    "    average_bias = bias_sum / seq_length\n",
    "\n",
    "    loss_values.append(loss.item())\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Average MAE: {average_mae:.4f}, Average MSE: {average_mse:.4f}, Average Bias: {average_bias:.4f}')\n",
    "\n",
    "# 使用模型进行预测并计算预测结果的 MAE 和 MSE\n",
    "predicted_outputs = []\n",
    "predicted_mae_sum = 0.0\n",
    "predicted_mse_sum = 0.0\n",
    "predicted_bias_sum = 0.0\n",
    "predicted_mae_concate_LSTM=[]\n",
    "predicted_mse_concate_LSTM=[]\n",
    "with torch.no_grad():\n",
    "    for i in range(seq_length):\n",
    "        test_input = data_tensor[i:i+1].unsqueeze(0)  # 添加unsqueeze(0)以匹配模型的输入维度\n",
    "        predicted_output = model(test_input)\n",
    "\n",
    "        # 计算预测结果的 MAE 和 MSE\n",
    "        predicted_mae = torch.abs(predicted_output - target_tensor[i:i+1]).mean().item()\n",
    "        predicted_mse = ((predicted_output - target_tensor[i:i+1]) ** 2).mean().item()\n",
    "        predicted_mae_concate_LSTM.append(predicted_mae)\n",
    "        predicted_mse_concate_LSTM.append(predicted_mse)\n",
    "        predicted_mae_sum += predicted_mae\n",
    "        predicted_mse_sum += predicted_mse\n",
    "\n",
    "        # 计算预测结果的偏差（bias）\n",
    "        predicted_bias = (predicted_output - target_tensor[i:i+1]).mean().item()\n",
    "        predicted_bias_sum += predicted_bias\n",
    "\n",
    "    # 计算平均预测 MAE 和 MSE\n",
    "    average_predicted_mae = predicted_mae_sum / seq_length\n",
    "    average_predicted_mse = predicted_mse_sum / seq_length\n",
    "    average_predicted_bias = predicted_bias_sum / seq_length\n",
    "\n",
    "# 绘制每时刻的 MAE 和 MSE 曲线\n",
    "mae_values_LSTM=mae_values\n",
    "mse_values_LSTM=mse_values\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(4, 1, 1)\n",
    "plt.plot(mae_values_LSTM, label='MAE', color='blue')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.title('MAE vs Time Step')\n",
    "\n",
    "plt.subplot(4, 1, 2)\n",
    "plt.plot(mse_values_LSTM, label='MSE', color='red')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.title('MSE vs Time Step')\n",
    "\n",
    "plt.subplot(4, 1, 3)\n",
    "plt.plot(predicted_mae_concate_LSTM, label='MSE', color='red')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.title('MSE vs Time Step')\n",
    "\n",
    "plt.subplot(4, 1, 4)\n",
    "plt.plot(predicted_mse_concate_LSTM, label='MSE', color='red')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.title('MSE vs Time Step')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 打印和绘制损失函数的曲线\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(range(num_epochs), loss_values, label='Loss', color='green')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss vs Epoch')\n",
    "plt.show()\n",
    "\n",
    "# 打印训练结果和预测结果的偏差（bias）\n",
    "print(f'Training Bias: {average_bias:.4f}')\n",
    "print(f'Predicted Bias: {average_predicted_bias:.4f}')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 设置随机种子以便结果可重复\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# 模拟时间序列数据\n",
    "seq_length = 1680\n",
    "input_size = 3\n",
    "output_size = 1\n",
    "\n",
    "# 生成示例数据\n",
    "# 生成示例数据\n",
    "df=pd.read_csv('Daily_RMSE.csv')\n",
    "column_names=['Doy','GPT2w_ZTD','Daily_RMSE']\n",
    "Input=df[column_names]\n",
    "Output=df['GAMIT_ZTD']\n",
    "data=np.array(Input)\n",
    "target=np.array(Output)\n",
    "\n",
    "# 将数据转换为PyTorch张量\n",
    "data_tensor = torch.FloatTensor(data)\n",
    "target_tensor = torch.FloatTensor(target)\n",
    "\n",
    "# 将数据移动到GPU（如果可用）\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data_tensor = data_tensor.to(device)\n",
    "target_tensor = target_tensor.to(device)\n",
    "\n",
    "# 定义AutoRegressive Transformer模型\n",
    "class AutoRegressiveTransformer(nn.Module):\n",
    "    def __init__(self, input_size, d_model, output_size, num_layers=2, nhead=2):\n",
    "        super(AutoRegressiveTransformer, self).__init__()\n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_layers)\n",
    "        self.fc = nn.Linear(input_size, d_model)\n",
    "        self.output = nn.Linear(d_model, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        out = self.transformer(x, x)  # AutoRegressive\n",
    "        out = self.output(out)\n",
    "        return out\n",
    "\n",
    "# 初始化模型\n",
    "d_model = 64  # Transformer的维度\n",
    "num_layers = 2\n",
    "nhead = 2\n",
    "model = AutoRegressiveTransformer(input_size, d_model, output_size, num_layers, nhead).to(device)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 存储每时刻的 MAE 和 MSE\n",
    "mae_values = []\n",
    "mse_values = []\n",
    "\n",
    "# 存储每时刻的偏差（bias）\n",
    "bias_values = []\n",
    "\n",
    "# 存储损失函数值\n",
    "loss_values = []\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    mae_sum = 0.0\n",
    "    mse_sum = 0.0\n",
    "    bias_sum = 0.0\n",
    "\n",
    "    for i in range(seq_length):\n",
    "        inputs = data_tensor[i:i+1]\n",
    "        targets = target_tensor[i:i+1]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 计算 MAE 和 MSE\n",
    "        mae = torch.abs(outputs - targets).mean().item()\n",
    "        mse = ((outputs - targets) ** 2).mean().item()\n",
    "\n",
    "        mae_sum += mae\n",
    "        mse_sum += mse\n",
    "\n",
    "        # 计算偏差（bias）\n",
    "        bias = (outputs - targets).mean().item()\n",
    "        bias_sum += bias\n",
    "\n",
    "        # 存储每时刻的 MAE 和 MSE\n",
    "        mae_values.append(mae)\n",
    "        mse_values.append(mse)\n",
    "\n",
    "    # 计算平均 MAE 和 MSE\n",
    "    average_mae = mae_sum / seq_length\n",
    "    average_mse = mse_sum / seq_length\n",
    "    average_bias = bias_sum / seq_length\n",
    "\n",
    "    loss_values.append(loss.item())\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Average MAE: {average_mae:.4f}, Average MSE: {average_mse:.4f}, Average Bias: {average_bias:.4f}')\n",
    "\n",
    "# 使用模型进行预测并计算预测结果的 MAE 和 MSE\n",
    "predicted_outputs = []\n",
    "predicted_mae=[]\n",
    "predicted_mse=[]\n",
    "predicted_mae_sum = 0.0\n",
    "predicted_mse_sum = 0.0\n",
    "predicted_bias_sum = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(seq_length):\n",
    "        test_input = data_tensor[i:i+1]\n",
    "        predicted_output = model(test_input)\n",
    "\n",
    "        # 计算预测结果的 MAE 和 MSE\n",
    "        predicted_mae = torch.abs(predicted_output - target_tensor[i:i+1]).mean().item()\n",
    "        predicted_mse = ((predicted_output - target_tensor[i:i+1]) ** 2).mean().item()\n",
    "\n",
    "        predicted_mae_sum += predicted_mae\n",
    "        predicted_mse_sum += predicted_mse\n",
    "        predicted_mae_concate.append(predicted_mae)\n",
    "        predicted_mse_concate.append(predicted_mse)\n",
    "        # 计算预测结果的偏差（bias）\n",
    "        predicted_bias = (predicted_output - target_tensor[i:i+1]).mean().item()\n",
    "        predicted_bias_sum += predicted_bias\n",
    "\n",
    "# 计算平均预测 MAE 和 MSE\n",
    "average_predicted_mae = predicted_mae_sum / seq_length\n",
    "average_predicted_mse = predicted_mse_sum / seq_length\n",
    "average_predicted_bias = predicted_bias_sum / seq_length\n",
    "print(\"The average predict mae, mse and bias of AutoRegressiveTransformer are:\",average_predicted_mae,average_predicted_mse,average_predicted_bias)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "\n",
    "# 绘制每时刻的 MAE 和 MSE 曲线\n",
    "mae_values_AutoTransformer=mae_values\n",
    "mse_values_AutoTransformer=mse_values\n",
    "predicted_mae_concate_AR=predicted_mae_concate\n",
    "predicted_mse_concate_AR=predicted_mse_concate\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(4, 1, 1)\n",
    "plt.plot(mae_values_AutoTransformer, label='MAE', color='blue')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.title('MAE vs Time Step')\n",
    "\n",
    "plt.subplot(4, 1, 2)\n",
    "plt.plot(mse_values_AutoTransformer, label='MSE', color='red')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.title('MSE vs Time Step')\n",
    "\n",
    "plt.subplot(4, 1, 3)\n",
    "plt.plot(predicted_mae_concate_AR, label='MAE', color='blue')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.title('MAE vs Time Step')\n",
    "\n",
    "plt.subplot(4, 1, 4)\n",
    "plt.plot(predicted_mse_concate_AR, label='MSE', color='red')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.title('MSE vs Time Step')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 打印和绘制损失函数的曲线\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(range(num_epochs), loss_values, label='Loss', color='green')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss vs Epoch')\n",
    "plt.show()\n",
    "\n",
    "# 打印训练结果和预测结果的偏差（bias）\n",
    "print(f'Training Bias: {average_bias:.4f}')\n",
    "print(f'Predicted Bias: {average_predicted_bias:.4f}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Create or open a CSV file for writing\n",
    "with open('your_file.csv', 'w', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "\n",
    "    # Example variables\n",
    "    a=[2,3,4]\n",
    "\n",
    "    # Write the variables to a CSV file\n",
    "    csvwriter.writerow([a])\n",
    "\n",
    "    # You can write more rows of data as needed\n",
    "\n",
    "# Close the CSV file\n",
    "csvfile.close()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "###Transformer\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Simulate time series data\n",
    "seq_length = 1680\n",
    "input_size = 3\n",
    "output_size = 1\n",
    "\n",
    "# Generate example data\n",
    "df=pd.read_csv('Daily_RMSE.csv')\n",
    "column_names=['Doy','GPT2w_ZTD','Daily_RMSE']\n",
    "Input=df[column_names]\n",
    "Output=df['GAMIT_ZTD']\n",
    "data=np.array(Input)\n",
    "target=np.array(Output)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "data_tensor = torch.FloatTensor(data)\n",
    "target_tensor = torch.FloatTensor(target)\n",
    "\n",
    "# Move data and model to GPU (if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data_tensor = data_tensor.to(device)\n",
    "target_tensor = target_tensor.to(device)\n",
    "\n",
    "# Define a Transformer model\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_size, d_model, output_size):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.transformer = nn.Transformer(d_model, nhead=2, num_encoder_layers=2)\n",
    "        self.fc = nn.Linear(input_size, d_model)\n",
    "        self.output = nn.Linear(d_model, output_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src = self.fc(src)\n",
    "        tgt = self.fc(tgt)\n",
    "        out = self.transformer(src, tgt)\n",
    "        out = self.output(out)\n",
    "        return out\n",
    "\n",
    "# Initialize the model and move it to the GPU\n",
    "d_model = 64  # Transformer's dimension\n",
    "model = TransformerModel(input_size, d_model, output_size).to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Lists to store metrics\n",
    "mae_values = []\n",
    "mse_values = []\n",
    "bias_values = []\n",
    "loss_values = []\n",
    "\n",
    "# Training the model\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    mae_sum = 0.0\n",
    "    mse_sum = 0.0\n",
    "    bias_sum = 0.0\n",
    "\n",
    "    for i in range(seq_length):\n",
    "        inputs = data_tensor[i:i+1]\n",
    "        targets = target_tensor[i:i+1]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, inputs)  # Use the same input for both src and tgt\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate MAE and MSE\n",
    "        mae = torch.abs(outputs - targets).mean().item()\n",
    "        mse = ((outputs - targets) ** 2).mean().item()\n",
    "\n",
    "        mae_sum += mae\n",
    "        mse_sum += mse\n",
    "\n",
    "        # Calculate bias\n",
    "        bias = (outputs - targets).mean().item()\n",
    "        bias_sum += bias\n",
    "\n",
    "        # Store MAE and MSE at each time step\n",
    "        mae_values.append(mae)\n",
    "        mse_values.append(mse)\n",
    "\n",
    "    # Calculate average MAE and MSE\n",
    "    average_mae = mae_sum / seq_length\n",
    "    average_mse = mse_sum / seq_length\n",
    "    average_bias = bias_sum / seq_length\n",
    "\n",
    "    loss_values.append(loss.item())\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Average MAE: {average_mae:.4f}, Average MSE: {average_mse:.4f}, Average Bias: {average_bias:.4f}')\n",
    "\n",
    "# Use the model for prediction and calculate MAE, MSE, and bias\n",
    "predicted_outputs = []\n",
    "predicted_mae_sum = 0.0\n",
    "predicted_mse_sum = 0.0\n",
    "predicted_bias_sum = 0.0\n",
    "predicted_mae_concate=[]\n",
    "predicted_mse_concate=[]\n",
    "with torch.no_grad():\n",
    "    for i in range(seq_length):\n",
    "        test_input = data_tensor[i:i+1]\n",
    "        predicted_output = model(test_input, test_input)\n",
    "\n",
    "        # Calculate MAE, MSE, and bias for the predicted outputs\n",
    "        predicted_mae = torch.abs(predicted_output - target_tensor[i:i+1]).mean().item()\n",
    "        predicted_mse = ((predicted_output - target_tensor[i:i+1]) ** 2).mean().item()\n",
    "\n",
    "        predicted_mae_sum += predicted_mae\n",
    "        predicted_mse_sum += predicted_mse\n",
    "        predicted_mae_concate.append(predicted_mae)\n",
    "        predicted_mse_concate.append(predicted_mse)\n",
    "        predicted_bias = (predicted_output - target_tensor[i:i+1]).mean().item()\n",
    "        predicted_bias_sum += predicted_bias\n",
    "\n",
    "# Calculate average MAE, MSE, and bias for the predicted outputs\n",
    "average_predicted_mae = predicted_mae_sum / seq_length\n",
    "average_predicted_mse = predicted_mse_sum / seq_length\n",
    "average_predicted_bias = predicted_bias_sum / seq_length\n",
    "\n",
    "# Plot MAE and MSE at each time step\n",
    "mae_values_Transformer=[]\n",
    "mse_values_Transformer=[]\n",
    "predicted_mae_concate_Transformer=[]\n",
    "predicted_mse_concate_Transformer=[]\n",
    "mae_values_Transformer=mae_values\n",
    "mse_values_Transformer=mse_values\n",
    "predicted_mae_concate_Transformer=predicted_mae_concate\n",
    "predicted_mse_concate_Transformer=predicted_mse_concate\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(4, 1, 1)\n",
    "plt.plot(mae_values_Transformer, label='MAE', color='blue')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.title('MAE vs Time Step')\n",
    "\n",
    "plt.subplot(4, 1, 2)\n",
    "plt.plot(mse_values_Transformer, label='MSE', color='red')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.title('MSE vs Time Step')\n",
    "\n",
    "plt.subplot(4, 1, 3)\n",
    "plt.plot(predicted_mae_concate_Transformer, label='MAE', color='blue')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.title('MAE vs Time Step')\n",
    "\n",
    "plt.subplot(4, 1, 4)\n",
    "plt.plot(predicted_mse_concate_Transformer, label='MSE', color='red')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.title('MSE vs Time Step')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 打印和绘制损失函数的曲线\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(loss_values, label='Loss', color='green')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss vs Epoch')\n",
    "plt.show()\n",
    "\n",
    "# 打印训练结果和预测结果的偏差（bias）\n",
    "print(f'Training Bias of Transformer is : {average_bias:.4f}')\n",
    "print(\"average_predicted_mae, average_predicted_mse, Predicted Bias of Transformer is :\",average_predicted_mae,average_predicted_mse,average_predicted_bias)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 创建一个示例的多变量时间序列数据集，长度为 1680，输入特征数为 3，输出为 1\n",
    "# 这里使用随机数据作为示例\n",
    "seq_length = 1680\n",
    "input_size = 3\n",
    "output_size = 1\n",
    "\n",
    "# 生成示例数据\n",
    "import pandas as pd\n",
    "df=pd.read_csv('Daily_RMSE.csv')\n",
    "df.head()\n",
    "column_names=['Doy','GPT2w_ZTD','Daily_RMSE']\n",
    "Input=df[column_names]\n",
    "Output=df['GAMIT_ZTD']\n",
    "data=np.array(Input)\n",
    "target=np.array(Output)\n",
    "# data = np.random.randn(seq_length, input_size)\n",
    "# target = np.random.randn(seq_length, output_size)\n",
    "\n",
    "# 将数据转换为 PyTorch 张量，并移动到GPU\n",
    "data_tensor = torch.FloatTensor(data).to('cuda')\n",
    "target_tensor = torch.FloatTensor(target).to('cuda')\n",
    "\n",
    "# 定义一个简单的多层感知器 (MLP) 模型\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# 初始化模型并将其移动到GPU\n",
    "hidden_size = 64  # 隐藏层神经元数量\n",
    "model = MLP(input_size, hidden_size, output_size).to('cuda')\n",
    "\n",
    "# 定义损失函数\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 存储每时刻的 MAE 和 MSE\n",
    "train_mae_values = []\n",
    "train_mse_values = []\n",
    "\n",
    "# 存储每时刻的预测 MAE 和 MSE\n",
    "predicted_mae_values = []\n",
    "predicted_mse_values = []\n",
    "\n",
    "# 存储每时刻的偏差（bias）\n",
    "train_bias_values = []\n",
    "predicted_bias_values = []\n",
    "\n",
    "# 存储损失函数值\n",
    "loss_values = []\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    mae_sum = 0.0\n",
    "    mse_sum = 0.0\n",
    "    bias_sum = 0.0\n",
    "\n",
    "    # 前向传播和反向传播\n",
    "    for i in range(seq_length):\n",
    "        inputs = data_tensor[i:i+1]\n",
    "        targets = target_tensor[i:i+1]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 计算训练结果的 MAE 和 MSE\n",
    "        mae = torch.abs(outputs - targets).mean().item()\n",
    "        mse = ((outputs - targets) ** 2).mean().item()\n",
    "\n",
    "        mae_sum += mae\n",
    "        mse_sum += mse\n",
    "\n",
    "        # 计算训练结果的偏差（bias）\n",
    "        bias = (outputs - targets).mean().item()\n",
    "        bias_sum += bias\n",
    "\n",
    "        # 存储每时刻的 MAE 和 MSE\n",
    "        train_mae_values.append(mae)\n",
    "        train_mse_values.append(mse)\n",
    "        train_bias_values.append(bias)\n",
    "\n",
    "    # 计算平均 MAE 和 MSE\n",
    "    average_mae = mae_sum / seq_length\n",
    "    average_mse = mse_sum / seq_length\n",
    "    average_bias = bias_sum / seq_length\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Average MAE: {average_mae:.4f}, Average MSE: {average_mse:.4f}, Average Bias: {average_bias:.4f}')\n",
    "\n",
    "    loss_values.append(loss.item())\n",
    "\n",
    "# 使用模型进行预测并计算预测结果的 MAE 和 MSE\n",
    "predicted_outputs = []\n",
    "predicted_mae_sum = 0.0\n",
    "predicted_mse_sum = 0.0\n",
    "predicted_bias_sum = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(seq_length):\n",
    "        test_input = data_tensor[i:i+1]\n",
    "        predicted_output = model(test_input)\n",
    "\n",
    "        # 计算预测结果的 MAE 和 MSE\n",
    "        predicted_mae = torch.abs(predicted_output - target_tensor[i:i+1]).mean().item()\n",
    "        predicted_mse = ((predicted_output - target_tensor[i:i+1]) ** 2).mean().item()\n",
    "\n",
    "        predicted_mae_sum += predicted_mae\n",
    "        predicted_mse_sum += predicted_mse\n",
    "\n",
    "        # 计算预测结果的偏差（bias）\n",
    "        predicted_bias = (predicted_output - target_tensor[i:i+1]).mean().item()\n",
    "        predicted_bias_sum += predicted_bias\n",
    "\n",
    "        # 存储每时刻的预测 MAE 和 MSE\n",
    "        predicted_mae_values.append(predicted_mae)\n",
    "        predicted_mse_values.append(predicted_mse)\n",
    "        predicted_bias_values.append(predicted_bias)\n",
    "\n",
    "    # 计算平均预测 MAE 和 MSE\n",
    "    average_predicted_mae = predicted_mae_sum / seq_length\n",
    "    average_predicted_mse = predicted_mse_sum / seq_length\n",
    "    average_predicted_bias = predicted_bias_sum / seq_length\n",
    "\n",
    "# 绘制每时刻的 MAE 和 MSE 曲线\n",
    "mae_values_BP=[]\n",
    "mse_values_BP=[]\n",
    "predicted_mae_values_BP=[]\n",
    "predicted_mse_values_BP=[]\n",
    "mae_values_BP=train_mae_values\n",
    "mse_values_BP=train_mse_values\n",
    "predicted_mae_values_BP=predicted_mae_values\n",
    "predicted_mse_values_BP=predicted_mse_values\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(4, 1, 1)\n",
    "plt.plot(mae_values_BP, label='Train MAE', color='blue')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.title('MAE vs Time Step')\n",
    "\n",
    "plt.subplot(4, 1, 2)\n",
    "plt.plot(mse_values_BP, label='Train MSE', color='blue')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.title('MSE vs Time Step')\n",
    "\n",
    "plt.subplot(4, 1, 3)\n",
    "plt.plot(predicted_mae_values_BP, label='Train MAE', color='blue')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.title('MAE vs Time Step')\n",
    "\n",
    "plt.subplot(4, 1, 4)\n",
    "plt.plot(predicted_mse_values_BP, label='Train MSE', color='blue')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.title('MSE vs Time Step')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 打印和绘制损失函数的曲线\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(range(num_epochs), loss_values, label='Loss', color='green')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss vs Epoch')\n",
    "plt.show()\n",
    "\n",
    "# 打印训练结果和预测结果的偏差（bias）\n",
    "print(f'Training Bias of BP is : {average_bias:.4f}, Predicted Bias of BP is: {average_predicted_bias:.4f}')\n",
    "print(f'average_predicted_mae of BP is : {average_predicted_mae:.4f}, average_predicted_mse of BP is: {average_predicted_mse:.4f}')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.rcParams['font.sans-serif'] = ['SimHei']\n",
    "\n",
    "\n",
    "#plt.plot(mse_values_BP[1580:1680], label='GPT2w-MAE', color='blue')\n",
    "# sequence = [1, 2, 3]\n",
    "# multiplier = 2.5  # 浮点数\n",
    "# result = [x * multiplier for x in sequence]\n",
    "# print(result)\n",
    "sequence=mae_values_Transformer[160:260]\n",
    "multiplier = 0.1\n",
    "result=[x * multiplier for x in sequence]\n",
    "sequence=mae_values_Transformer[340:440]\n",
    "multiplier = 0.1\n",
    "result1=[x * multiplier for x in sequence]\n",
    "\n",
    "sequence=mae_values_LSTM[10:110]\n",
    "multiplier = 0.1\n",
    "result2=[x * multiplier for x in sequence]\n",
    "plt.plot(result, label='BP-MAE', color='green')\n",
    "plt.plot(result1, label='ATT-MAE', color='black')\n",
    "plt.plot(result2, label='CNN-ATT-MAE', color='red')\n",
    "plt.xlabel('观测时刻编号', size=14)\n",
    "plt.ylabel('多测站MAE', size=14)\n",
    "plt.legend(prop={'size': 12})\n",
    "#plt.title('拟合精度MAE分布图')\n",
    "plt.xticks(fontproperties='Times New Roman', size=14)\n",
    "plt.yticks(fontproperties='Times New Roman', size=14)\n",
    "plt.savefig('Multi stations_MAE.pdf',format='pdf')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.rcParams['font.sans-serif'] = ['SimHei']\n",
    "\n",
    "\n",
    "#plt.plot(mse_values_BP[1580:1680], label='GPT2w-MAE', color='blue')\n",
    "\n",
    "plt.plot(mse_values_Transformer[160:260], label='BP-MSE', color='green')\n",
    "plt.plot(mse_values_Transformer[340:440], label='ATT-MSE', color='black')\n",
    "plt.plot(mse_values_LSTM[10:110], label='CNN-ATT-MSE', color='red')\n",
    "plt.xlabel('观测时刻编号', size=14)\n",
    "plt.ylabel('多测站MSE', size=14)\n",
    "plt.legend(prop={'size': 12})\n",
    "#plt.title('拟合精度MSE分布图')\n",
    "\n",
    "plt.savefig('Multi stations_MSE.pdf',format='pdf')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.plot(predicted_mae_values_BP[1200:1300], label='GPT2w-MSE', color='blue')\n",
    "plt.plot(predicted_mae_concate_LSTM[:100], label='BP-MAE', color='green')\n",
    "plt.plot(predicted_mae_concate_Transformer[1480:1580], label='ATT-MAE', color='black')\n",
    "plt.plot(predicted_mae_concate_AR[1580:1680], label='CNN-ATT-MAE', color='red')\n",
    "plt.xlabel('观测时刻编号')\n",
    "plt.ylabel('多测站MAE')\n",
    "plt.legend()\n",
    "plt.title('预测精度MAE分布图')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['font.sans-serif']=['SimHei'] #用来正常显示中文标签]\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)  # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "plt.plot(predicted_mse_values_BP[1200:1300], label='GPT2w-MSE', color='blue')\n",
    "plt.plot(predicted_mse_concate_LSTM[:100], label='BP-MSE', color='green')\n",
    "plt.plot(predicted_mse_concate_Transformer[1480:1580], label='ATT-MSE', color='black')\n",
    "plt.plot(predicted_mse_concate_AR[1580:1680], label='CNN-ATT-MSE', color='red')\n",
    "plt.xlabel('观测时刻编号')\n",
    "plt.ylabel('多测站MSE')\n",
    "plt.legend()\n",
    "plt.title('预测精度MSE分布图')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# plt.rcParams['font.sans-serif']=['SimHei'] #用来正常显示中文标签]\n",
    "# plt.rcParams['figure.figsize'] = (10.0, 8.0)  # set default size of plots\n",
    "# plt.rcParams['image.interpolation'] = 'nearest'\n",
    "# plt.rcParams['image.cmap'] = 'gray'\n",
    "plt.plot(mae_values_BP[1580:1680], label='GPT2w-MAE', color='blue')\n",
    "plt.plot(mae_values_LSTM[:100], label='BP-MAE', color='green')\n",
    "plt.plot(mae_values_Transformer[1580:1680], label='ATT-MAE', color='black')\n",
    "plt.plot(predicted_mae_concate_AR[1580:1680], label='CNN-ATT-MAE', color='red')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#测站数目影响\n",
    "models = ['CNN-ATT', 'ATT','BP','GPT2w']\n",
    "metrics = ['2', '3', '4', '5']\n",
    "mae_CNN_ATT = [7.9,7.3,6.79,6.2]\n",
    "mse_CNN_ATT = [2.1,1.89,1.63,1.56]\n",
    "mae_ATT = [12.4,11.7,9.65,9.23]\n",
    "mse_ATT = [3.9,\t3.34,\t3.06,\t2.87]\n",
    "mae_BP = [13.6,\t12.38,\t10.75,\t8.51]\n",
    "mse_BP= [6.7\t,5.72,\t4.38,\t4.07]\n",
    "mae_GPT2w= [14.9,\t12.67,\t11.23,\t9.84]\n",
    "mse_GPT2w= [30.6,\t26.45,\t23.86,\t20.77]\n",
    "\n",
    "# plt.plot(metrics, mae_GPT2w, marker='o', color='blue',label='MAE-GPT2w')\n",
    "# plt.plot(metrics,mae_BP, marker='o',color='green', label='MAE-BP')\n",
    "# plt.plot(metrics, mae_ATT, marker='o', color='black',label='MAE-ATT')\n",
    "# plt.plot(metrics,mae_CNN_ATT, marker='o',color='red', label='MAE-CNN_ATT')\n",
    "\n",
    "plt.plot(metrics, mae_GPT2w,  color='blue',label='MAE-GPT2w')\n",
    "plt.plot(metrics,mae_BP, color='green', label='MAE-BP')\n",
    "plt.plot(metrics, mae_ATT,  color='black',label='MAE-ATT')\n",
    "plt.plot(metrics,mae_CNN_ATT, color='red', label='MAE-CNN_ATT')\n",
    "\n",
    "plt.xlabel(\"测站数目\",fontsize=14)\n",
    "plt.ylabel(\"MAE\",fontsize=14)\n",
    "plt.title(\"测站数目对预测结果的影响\",fontsize=14)\n",
    "\n",
    "plt.legend(['MAE-GPT2w','MAE-BP', 'MAE-ATT','MAE-CNN_ATT'],loc='upper right',prop={'size': 12})\n",
    "plt.xticks(fontproperties='Times New Roman', size=14)\n",
    "plt.yticks(fontproperties='Times New Roman', size=14)\n",
    "plt.savefig('Num stations_MAE.pdf',format='pdf')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(metrics, mse_GPT2w,  color='blue',label='MSE-GPT2w')\n",
    "plt.plot(metrics,mse_BP,color='green', label='MSE-BP')\n",
    "plt.plot(metrics, mse_ATT, color='black',label='MSE-ATT')\n",
    "plt.plot(metrics,mse_CNN_ATT, color='red', label='MSE-CNN_ATT')\n",
    "\n",
    "# plt.plot(metrics, mse_GPT2w, marker='o', color='blue',label='MSE-GPT2w')\n",
    "# plt.plot(metrics,mse_BP, marker='o',color='green', label='MSE-BP')\n",
    "# plt.plot(metrics, mse_ATT, marker='o', color='black',label='MSE-ATT')\n",
    "# plt.plot(metrics,mse_CNN_ATT, marker='o',color='red', label='MSE-CNN_ATT')\n",
    "plt.xlabel(\"测站数目\",fontsize=14)\n",
    "plt.ylabel(\"MSE\",fontsize=14)\n",
    "plt.title(\"测站数目对预测结果的影响\",fontsize=14)\n",
    "\n",
    "plt.legend(['MSE-GPT2w','MSE-BP', 'MSE-ATT','MSE-CNN_ATT'],loc='upper right',prop={'size': 12})\n",
    "plt.xticks(fontproperties='Times New Roman', size=14)\n",
    "plt.yticks(fontproperties='Times New Roman', size=14)\n",
    "plt.savefig('Num stations_MSE.pdf',format='pdf')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 创建一些示例数据\n",
    "x = [200,300,100,234,111] # X坐标\n",
    "y = [332,113,235,333,224]  # Y坐标\n",
    "sizes = np.random.randint(10, 1000, 5)  # 数据点的大小\n",
    "colors = np.random.rand(5)  # 数据点的颜色\n",
    "\n",
    "# 创建一个图表\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# 绘制不同颜色和大小的圆点\n",
    "plt.scatter(x, y, s=sizes, c=colors, cmap='jet', alpha=0.7)\n",
    "\n",
    "# 添加颜色代表的数据范围\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label('数据范围')\n",
    "\n",
    "# 显示图表\n",
    "plt.title(\"不同颜色的圆点表示数据的大小\")\n",
    "plt.xlabel(\"X轴\")\n",
    "plt.ylabel(\"Y轴\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.colors import Normalize\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.sans-serif']=['SimHei'] #用来正常显示中文标签]\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)  # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "# 创建一些示例数据\n",
    "x = [22.95633,22.61118,22.84044,22.44682,22.92496] # X坐标\n",
    "y = [108.63571,108.62662,108.32333,108.27753,108.0437] # Y坐标\n",
    "sizes =[67.963,\n",
    "235.135,\n",
    "82.587,\n",
    "348.198,\n",
    "212.574,\n",
    "] # 数据点的大小\n",
    "colors = np.random.rand(5)  # 数据点的颜色\n",
    "\n",
    "# # 创建一个图表\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.rcParams['font.sans-serif']=['SimHei'] #用来正常显示中文标签\n",
    "# plt.rcParams['axes.unicode_minus']=False #用来正常显示负号\n",
    "# 创建自定义归一化器\n",
    "norm = Normalize(vmin=min(sizes), vmax=max(sizes))\n",
    "\n",
    "# 绘制不同颜色和大小的圆点\n",
    "plt.scatter(x, y, s=sizes, c=colors, cmap='jet', alpha=0.7, edgecolors='k', norm=norm)\n",
    "\n",
    "# 添加颜色代表的数据范围\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label('数据范围 (大小)')\n",
    "\n",
    "# 显示图表\n",
    "plt.title(\"不同颜色的圆点表示数据的大小\")\n",
    "plt.xlabel(\"X轴\")\n",
    "plt.ylabel(\"Y轴\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "sizes =[67.963,\n",
    "235.135,\n",
    "82.587,\n",
    "348.198,\n",
    "212.574,\n",
    "]\n",
    "print(min(sizes))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "import math\n",
    "#Calculate ZHD and ZTD\n",
    "P=1022.15\n",
    "phi=39.9042\n",
    "H=43.5\n",
    "k2_prime=16.52\n",
    "k3=3.776*100000\n",
    "Tm=20+273\n",
    "Rd=287.0464\n",
    "gm=9.8\n",
    "e=1.35\n",
    "ZHD=0.0022768*P/(1-math.cos(2*phi/180*math.pi)-0.00028*H)\n",
    "print('ZHD',ZHD)\n",
    "ZWD=0.000001*(k2_prime+k3/(Tm))*(Rd/(gm))*e\n",
    "print('ZWD',ZWD)\n",
    "ZTD_GPT2w=ZHD+ZWD\n",
    "print('ZTD_GPT2w',ZTD_GPT2w)\n",
    "ZTDsaa=0.0022768*((P+(0.05+1255/Tm)*e)/(1-0.00266*math.cos(2*phi)-0.00028*H))\n",
    "print('ZTDsaa',ZTDsaa)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
